{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting University Admissions\n",
    "In this jupyter notebook I will predict university admissions based\n",
    "on a dataset found here: https://www.kaggle.com/tanmoyie/us-graduate-schools-admission-parameters.\n",
    "\n",
    "All the methods implemented from scratch use mini-batch stochastic gradient descent while all the scikit learn\n",
    "methods use regular stochastic gradient descent.\n",
    "\n",
    "From scratch:\n",
    "1. Linear Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Heuristic step size\n",
    "    * Step size determined with Adagrad\n",
    "2. Polynomial Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Heuristic step size\n",
    "    * Step size determined with Adagrad\n",
    "\n",
    "With scikit-learn:\n",
    "1. Linear Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Inversely scaled step size\n",
    "2. Ridge Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Inversely scaled step size\n",
    "3. Lasso Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Inversely scaled step size\n",
    "3. Polynomial Regression\n",
    "    * Fixed step size of 0.01\n",
    "    * Inversely scaled step size\n",
    "4. Polynomial Ridge Regression\n",
    "5. Polynomial Lasso Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AdmissionsData.csv')\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next part we are interested in the last column because it tells us if each individual feature is\n",
    "positively correlated to the chance of admission. In other words we can check if the feature can help us predict the\n",
    "chance of admission. We can see that GRE Score, CGPA, and TOEFL Score all strongly correlate with the chance of admission."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can further explore the relationship between variables using scatter plots to visualize correlations and outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.plot(x=data.columns[-1], y='GRE Score', kind='scatter')\n",
    "data.plot(x='GRE Score', y='TOEFL Score', kind='scatter')\n",
    "data.plot(x='GRE Score', y='CGPA', kind='scatter')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading and cleaning the data\n",
    "There is no data cleaning to do since this dataset came from Kaggle and is already cleaned.\n",
    "##### Load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "filename = 'AdmissionsData.csv'\n",
    "\n",
    "# [1:, 1:] removes the first row (column names) and the first column (index)\n",
    "dataset = np.genfromtxt(filename, delimiter=',')[1:, 1:]\n",
    "print(dataset[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make predictions with only the GRE Score, CGPA, and TOEFL Score features run the code block below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     dataset = np.delete(dataset, 2, 1)\n",
    "# dataset = np.delete(dataset, 3, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Split the data into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "numFeatures = dataset.shape[1] - 1\n",
    "randomIndices = np.random.choice(dataset.shape[0], len(dataset), replace=False)\n",
    "Xtrain = dataset[randomIndices[:350], :numFeatures]\n",
    "Ytrain = dataset[randomIndices[:350], numFeatures]\n",
    "\n",
    "Xtest = dataset[randomIndices[350:], :numFeatures]\n",
    "Ytest = dataset[randomIndices[350:], numFeatures]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Normalize the features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(Xtrain.shape[1]):\n",
    "    Xtrain[:, i] = np.divide(Xtrain[:, i], np.max(np.abs(Xtrain[:, i])))\n",
    "    Xtest[:, i] = np.divide(Xtest[:, i], np.max(np.abs(Xtest[:, i])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Augment data with column of 1's (acts as intercept weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xtrain = np.hstack((Xtrain, np.ones((Xtrain.shape[0], 1))))\n",
    "Xtest = np.hstack((Xtest, np.ones((Xtest.shape[0], 1))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Linear Regression\n",
    "The following methods all use mini-batch stochastic gradient descent but have distinct\n",
    "methods of determining step size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialize values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batchSize = 50\n",
    "epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainLinReg(Xtrain, Ytrain, stepSizeChoice):\n",
    "    numSamples = Xtrain.shape[0]\n",
    "    numFeatures = Xtrain.shape[1]\n",
    "    weights = np.zeros(numFeatures)\n",
    "\n",
    "    if stepSizeChoice == \"heuristic\":\n",
    "        gBar = 1\n",
    "    elif stepSizeChoice == \"adagrad\":\n",
    "        gBar = np.zeros(numFeatures)\n",
    "\n",
    "    # loop through epochs\n",
    "    for i in range(epochs):\n",
    "        shuffle = list(zip(Xtrain, Ytrain))\n",
    "        random.shuffle(shuffle)\n",
    "        X, Y = zip(*shuffle)\n",
    "        Xtrain = np.array(list(X))\n",
    "        Ytrain = np.array(list(Y))\n",
    "\n",
    "        index = 0\n",
    "        for j in range(numSamples//batchSize):\n",
    "            gradient = np.zeros(numFeatures)\n",
    "\n",
    "            # calculate the gradient for a given batch\n",
    "            for x in range(index, index + batchSize):\n",
    "                # (xw - y)x sgd update rule\n",
    "                gradient += np.dot(np.dot(Xtrain[x], weights) - Ytrain[x], Xtrain[x])\n",
    "            gradient = (1 / batchSize) * gradient\n",
    "\n",
    "            # choose step size\n",
    "            if stepSizeChoice == \"heuristic\":\n",
    "                gBar = gBar + ((1/8) * np.sum(np.abs(gradient)))\n",
    "                stepSize = (1 + gBar) ** (-1)\n",
    "            elif stepSizeChoice == \"adagrad\":\n",
    "                gBar = gBar + np.square(gradient)\n",
    "                stepSize = np.divide(np.ones(gBar.shape[0]), np.sqrt(gBar))\n",
    "            else:\n",
    "                stepSize = 0.01\n",
    "\n",
    "            # update weights\n",
    "            weights = weights - np.multiply(stepSize, gradient)\n",
    "\n",
    "            index += batchSize\n",
    "\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Predict and Calculate Error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meanTrain = np.mean(Ytrain)\n",
    "meanPredict = np.ones((Xtest.shape[0],)) * meanTrain\n",
    "meanErr = np.square(np.subtract(meanPredict, Ytest)).mean()\n",
    "print('Baseline Mean Squared Error:', meanErr)\n",
    "\n",
    "linRegErrors = []\n",
    "stepType = ['fixed', 'heuristic', 'adagrad']\n",
    "for step in stepType:\n",
    "    weights = trainLinReg(Xtrain, Ytrain, step)\n",
    "    predictions = np.dot(Xtest, weights)\n",
    "    linRegErrors.append(np.subtract(predictions, Ytest))\n",
    "    error = np.square(np.subtract(predictions, Ytest)).mean()\n",
    "    print(f\"Mean Squared Error with {step} step size: \", error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Polynomial Regression\n",
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transforms Xtrain data to fit a polynomial with degree 2\n",
    "def transform(Xdata):\n",
    "    x = list(Xdata)\n",
    "    x.reverse()\n",
    "    # loop starting from index 1 since the first term (1) is a bias term that isn't an actual feature\n",
    "    new_x = [i for i in x]\n",
    "    for i in range(1, len(x)):\n",
    "        new_x.append(x[i]**2)\n",
    "        for j in range(i+1, len(x)):\n",
    "            new_x.append(x[i] * x[j])\n",
    "    return np.array(new_x)\n",
    "\n",
    "\n",
    "polyXtrain = []\n",
    "for i in range(len(Xtrain)):\n",
    "    polyXtrain.append(transform(Xtrain[i]))\n",
    "polyXtrain = np.array(polyXtrain)\n",
    "\n",
    "polyXtest = []\n",
    "for i in range(len(Xtest)):\n",
    "    polyXtest.append(transform(Xtest[i]))\n",
    "polyXtest = np.array(polyXtest)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict and Calculate Error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meanTrain = np.mean(Ytrain)\n",
    "meanPredict = np.ones((polyXtest.shape[0],)) * meanTrain\n",
    "meanErr = np.square(np.subtract(meanPredict, Ytest)).mean()\n",
    "print('Baseline Mean Squared Error:', meanErr)\n",
    "\n",
    "\n",
    "stepType = ['fixed', 'heuristic', 'adagrad']\n",
    "errors = []\n",
    "for step in stepType:\n",
    "    weights = trainLinReg(polyXtrain, Ytrain, step)\n",
    "    predictions = np.dot(polyXtest, weights)\n",
    "    errors.append(np.subtract(predictions, Ytest))\n",
    "    MSE = np.square(np.subtract(predictions, Ytest)).mean()\n",
    "    print(f\"Mean Squared Error with {step} step size: \", MSE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Error Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for err in errors:\n",
    "    plt.hist(err, bins=70)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare models and pick the best one with a certain level of confidence using the paired t-test.\n",
    "Less run an example paired t-test to compare polynomial models trained with fixed vs Adagrad step size.\n",
    "\n",
    "Our null hypothesis is that the performance of the fixed step size model is the same as the performance of the Adagrad\n",
    "step size model (no significant difference in error). The alternative hypothesis is that the Adagrad model performed\n",
    "better (significant difference in error).\n",
    "\n",
    "We are assuming the data is i.i.d. and as we can see from the error charts above the errors are approximately normally\n",
    "distributed. Furthermore, the mean and variance of the fixed step size error and Adagrad step size error are relatively\n",
    "the same. Thus, we can calculate the t-statistic and subsequent p-value to determine which hypothesis is more probable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import t\n",
    "\n",
    "m = len(errors[0])  # dimension of the test set\n",
    "difference = np.subtract(linRegErrors[0], errors[2])\n",
    "d = (1 / m) * np.sum(difference)\n",
    "y = np.full(difference.shape, d)\n",
    "temp = (1 / (m - 1)) * np.sum(np.square(np.subtract(difference, y)))\n",
    "\n",
    "s = math.sqrt(temp)\n",
    "k = d / (s / math.sqrt(m))\n",
    "\n",
    "pVal = 1 - t.cdf(k, m-1)\n",
    "\n",
    "print(pVal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the p-value is > 0.05 we cannot say with 95% confidence that the difference in performance between Adagrad and\n",
    "fixed step size models is significant. This was simply an example and much more rigorous analysis is necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Scikit-Learn\n",
    "## Reading and cleaning the data\n",
    "##### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "filename = 'AdmissionsData.csv'\n",
    "dataset = genfromtxt(filename, delimiter=',')[1:, 1:]\n",
    "X = dataset[:, :7]\n",
    "Y = np.concatenate(dataset[:, 7:], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Split data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, train_size=0.7, shuffle=True)\n",
    "print(Xtrain[:5])\n",
    "print(Ytrain[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Normalize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.fit_transform(Xtest)\n",
    "print(Xtrain[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train\n",
    "### Linear Regression\n",
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "fixedRegressor = SGDRegressor(loss='squared_loss', penalty='None', fit_intercept=True, shuffle=True,\n",
    "                         learning_rate='constant', eta0=0.01, max_iter=100)\n",
    "fixedRegressor.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invRegressor = SGDRegressor(loss='squared_loss', penalty='None', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "invRegressor.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ridge Regression\n",
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixedL2Regressor = SGDRegressor(loss='squared_loss', penalty='l2', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='constant', max_iter=100, eta0=0.01)\n",
    "fixedL2Regressor.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invL2Regressor = SGDRegressor(loss='squared_loss', penalty='l2', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "invL2Regressor.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lasso Regression\n",
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixedL1Regressor = SGDRegressor(loss='squared_loss', penalty='l1', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='constant', max_iter=100, eta0=0.01)\n",
    "fixedL1Regressor.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invL1Regressor = SGDRegressor(loss='squared_loss', penalty='l1', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "invL1Regressor.fit(Xtrain, Ytrain)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Polynomial Regression\n",
    "##### Transform data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "Xtrain = poly.fit_transform(Xtrain)\n",
    "Xtest = poly.fit_transform(Xtest)\n",
    "\n",
    "types = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "polyFixedReg = SGDRegressor(loss='squared_loss', penalty='None', fit_intercept=True, shuffle=True,\n",
    "                         learning_rate='constant', eta0=0.01, max_iter=100)\n",
    "polyFixedReg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Regression with fixed step size', polyFixedReg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polyInvReg = SGDRegressor(loss='squared_loss', penalty='None', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "polyInvReg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Regression with inversely scaled step size', polyInvReg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polynomial Ridge Regression\n",
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polyFixedL2Reg = SGDRegressor(loss='squared_loss', penalty='l2', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='constant', max_iter=100, eta0=0.01)\n",
    "polyFixedL2Reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Ridge Regression with fixed step size', polyFixedL2Reg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polyInvL2Reg = SGDRegressor(loss='squared_loss', penalty='l2', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "polyInvL2Reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Ridge Regression with inversely scaled step size', polyInvL2Reg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Polynomial Lasso Regression\n",
    "##### Fixed Step Size = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polyFixedL1Reg = SGDRegressor(loss='squared_loss', penalty='l1', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='constant', max_iter=100, eta0=0.01)\n",
    "polyFixedL1Reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Lasso Regression with a fixed step size', polyFixedL1Reg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Inversely Scaled Step Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polyInvL1Reg = SGDRegressor(loss='squared_loss', penalty='l1', fit_intercept=True, shuffle=True,\n",
    "                            learning_rate='invscaling', max_iter=100)\n",
    "polyInvL1Reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "types.append(('Polynomial Lasso Regression with inversely scaled step size', polyInvL1Reg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(Xtrain, Ytrain)\n",
    "dummyPredictions = dummy.predict(Xtest)\n",
    "print('Mean Error:', np.square(np.subtract(dummyPredictions, Ytest)).mean())\n",
    "\n",
    "for regressor in types:\n",
    "    predictions = regressor[1].predict(Xtest)\n",
    "    scores = cross_val_score(regressor[1], Xtrain, Ytrain, cv=5)\n",
    "    print(f\"{scores.mean()} accuracy with standard deviation of {scores.std()}\")\n",
    "    print('Mean Squared Error for ' + regressor[0] + ':', mean_squared_error(predictions, Ytest))\n",
    "    print('')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}